[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Paper | Code\n\n\n\n\nStrongly Topology-preserving GNNs for Brain Graph Super-resolution\n\n\nPragya Singh and Islem Rekik  PRIME-MICCAI 2024\n\n\nBrain graph super-resolution is an under-explored yet highly relevant task in network neuroscience. It circumvents the need for costly and time-consuming medical imaging data collection, preparation, and processing. Current super-resolution methods leverage graph neural networks (GNNs) thanks to their ability to natively handle graph-structured datasets. However, most GNNs perform node feature learning, which presents two significant limitations: (1) they require computationally expensive methods to learn complex node features capable of inferring connectivity strength or edge features, which do not scale to larger graphs; and (2) computations in the node space fail to adequately capture higher-order brain topologies such as cliques and hubs. However, numerous studies have shown that brain graph topology is crucial in identifying the onset and presence of various neurodegenerative disorders like Alzheimer’s and Parkinson’s. Motivated by these challenges and applications, we propose our Strongly Topology-Preserving GNN framework for Brain Graph Super-Resolution (STP-GSR). It is the first graph super-resolution architecture to perform representation learning in higher-order topological space. Specifically, using the primal-dual graph formulation from graph theory, we develop an efficient mapping from the edge space of our low-resolution (LR) brain graphs to the node space of a high-resolution (HR) dual graph. This approach ensures that node-level computations on this dual graph correspond naturally to edge-level learning on our HR brain graphs, thereby enforcing strong topological consistency within our framework. Additionally, our framework is GNN layer agnostic and can easily learn from smaller, scalable GNNs, significantly reducing computational requirements. We comprehensively benchmark our framework across seven key topological measures and observe that it significantly outperforms the previous state-of-the-art methods and baselines.\n\n\n\n\n\n\n\nPaper | Code\n\n\n\n\nGroup-invariant machine learning on the Kreuzer-Skarke dataset\n\n\nChristian Ewert, Sumner Magruder, Vera Maiboroda, Yueyang Shen, Pragya Singh, Daniel Platt  Physics Letters B\n\n\nWe use supervised machine learning to predict Hodge numbers for Calabi-Yau threefolds encoded by reflexive polyhedra. The Hodge number is invariant to the order of the vertices and the swapping of axes. Incorporating these properties, i.e. the invariance of column and row permutations for a matrix containing the polyhedron’s vertices, promises better performance for Hodge number prediction. On a medium-sized subset of the Kreuzer-Skarke dataset, we train and evaluate approaches with different degrees of invariance. Our comparison shows that machine learning models incorporating symmetries actually outperform models that do not, with our best model achieving almost 97% accuracy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pragya Singh",
    "section": "",
    "text": "Hi, I am an incoming PhD student at the University of Oxford under the Intelligent Earth CDT program. I am interested in geometric and topological machine learning, especially their application to natural sciences. During my PhD, I would like to dive deeper into the intersection of AI and physics, not only to model complex physical processes, but also to understand how these models encode phsyical laws (which utlimately may provide insights into improving the modelling aspect).\n\nEducation\nImperial College London | London, UK\nMSc in Artificial Intelligence | Sept 2023 - Sept 2024\nIIT Roorkee | Uttrakhand, India\nB.Tech in Engineering Physics | July 2016 - May 2020\n\n\nExperience\nGoldman Sachs | Bengaluru, India\nQuantitative Strategist | July 2020 - July 2023"
  }
]